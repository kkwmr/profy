# Profy: AIã«ã‚ˆã‚‹ãƒ”ã‚¢ãƒæ¼”å¥è©•ä¾¡ãƒ»æŒ‡å°ã‚·ã‚¹ãƒ†ãƒ 

**Profy** ã¯ã€MERTï¼ˆMusic-understanding Efficient pre-trained Transformerï¼‰ã¨ç‰©ç†ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦ã€ãƒ”ã‚¢ãƒæ¼”å¥ã®è©³ç´°ãªåˆ†æã¨å…·ä½“çš„ãªæ”¹å–„æŒ‡å°ã‚’æä¾›ã™ã‚‹AIã‚³ãƒ¼ãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®æ¨™

### ç¾åœ¨ã®æ”¹å–„çŠ¶æ³
- **Phase 1 (å®Œäº†)**: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®åŸºæœ¬çš„ãªå•é¡Œã‚’ä¿®æ­£
  - âœ… åˆ†é¡å™¨ãƒã‚¤ã‚¢ã‚¹ã®åˆæœŸåŒ–ä¿®æ­£ï¼ˆå°‘æ•°ã‚¯ãƒ©ã‚¹ã«é©åˆ‡ãªåˆæœŸå€¤ï¼‰
  - âœ… Focal Lossã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾ç­–ï¼ˆÎ±=0.55/0.45, Î³=1.0ï¼‰
  - âœ… å­¦ç¿’ç‡ã®æœ€é©åŒ–ï¼ˆ5e-5ï¼‰ã¨Cosine Annealingã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
  
- **Phase 2 (å®Œäº†)**: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ”¹å–„
  - âœ… éŸ³å£°ï¼ˆ24kHzï¼‰ãƒ»ã‚»ãƒ³ã‚µãƒ¼ï¼ˆ1000Hzï¼‰ãƒ‡ãƒ¼ã‚¿ã®å³å¯†ãªæ™‚é–“åŒæœŸ
  - âœ… note.jsonãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®éŸ³ç¬¦ãƒ¬ãƒ™ãƒ«ç‰¹å¾´æŠ½å‡ºï¼ˆNoteFeatureExtractorï¼‰
  - âœ… éŸ³æ¥½çš„è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã‚¿ã‚¤ãƒŸãƒ³ã‚°ç²¾åº¦ã€ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ä¸€è²«æ€§ã€ãƒ¬ã‚¬ãƒ¼ãƒˆå“è³ªï¼‰

### ğŸ¼ ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´
- **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆåˆ†æ**: éŸ³å£°ï¼ˆ24kHzï¼‰ã¨88éµã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ1000Hzï¼‰ã®åŒæœŸå‡¦ç†
- **ç‰©ç†çš„æ¼”å¥è§£æ**: ã‚­ãƒ¼æŠ¼ä¸‹æ·±åº¦ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ã®è©³ç´°æ¸¬å®šï¼ˆmmå˜ä½ï¼‰
- **å…·ä½“çš„ãªæ”¹å–„æŒ‡å°**: ã©ã®éŸ³ç¬¦ã§ã€ã©ã®ã‚ˆã†ãªæŠ€è¡“çš„å•é¡ŒãŒã‚ã‚‹ã‹ã‚’ç‰¹å®š

### ğŸ§  æ”¹è‰¯ã•ã‚ŒãŸAIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **éšå±¤çš„MERTç‰¹å¾´æŠ½å‡º**: è¤‡æ•°å±¤ã‹ã‚‰ã®ç‰¹å¾´ã‚’çµ±åˆåˆ©ç”¨
- **æ™‚é–“åŒæœŸãƒ¡ã‚«ãƒ‹ã‚ºãƒ **: éŸ³å£°ã¨ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªå¯¾å¿œä»˜ã‘
- **Physics-Informedè¨­è¨ˆ**: ç‰©ç†çš„åˆ¶ç´„ã‚’è€ƒæ…®ã—ãŸå­¦ç¿’

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-username/profy.git
cd profy

# condaç’°å¢ƒã®ä½œæˆ
conda create -n profy python=3.9 -y
conda activate profy

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### å®Ÿè¡Œæ–¹æ³•

```bash
# ãƒ‡ãƒ¼ã‚¿æº–å‚™
python scripts/prepare_data.py

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ”¹å–„ã•ã‚ŒãŸconfigä½¿ç”¨ï¼‰
python scripts/train.py --config configs/config.yaml --output-dir results/$(date +%Y%m%d_%H%M%S)_improved --data-dir data/full_splits

# ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
python scripts/evaluate.py --model results/MODEL_DIR/best_model.pth --data-dir data/full_splits --output-dir results/evaluation

# çµæœã®å¯è¦–åŒ–
python scripts/visualize.py
```

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
profy/
â”œâ”€â”€ configs/            # âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ config.yaml     # åŸºæœ¬è¨­å®š
â”œâ”€â”€ src/               # ğŸ’» ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ models/        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ data/          # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
â”‚   â”œâ”€â”€ evaluation/    # è©•ä¾¡é–¢é€£
â”‚   â””â”€â”€ visualization/ # å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«
â”œâ”€â”€ scripts/           # ğŸ”§ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ train.py       # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
â”‚   â”œâ”€â”€ evaluate.py    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
â”‚   â”œâ”€â”€ prepare_data.py # ãƒ‡ãƒ¼ã‚¿æº–å‚™
â”‚   â””â”€â”€ visualize.py   # çµæœå¯è¦–åŒ–
â”œâ”€â”€ data/              # ğŸ“Š å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ playdata/      # æ¼”å¥ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ full_splits/   # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
â”‚   â””â”€â”€ all_answer_summary_df.csv # è©•ä¾¡ã‚¹ã‚³ã‚¢
â”œâ”€â”€ results/           # ğŸ“ˆ å®Ÿé¨“çµæœ
â”‚   â””â”€â”€ YYYYMMDD_HHMMSS_training/ # å„è¨“ç·´ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµæœ
â”œâ”€â”€ logs/              # ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç¾åœ¨ã¯ç©ºï¼‰
â”œâ”€â”€ README.md          # ğŸ“– ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ WORK_INSTRUCTIONS.md # ğŸ“‹ é–‹ç™ºã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
â””â”€â”€ requirements.txt   # ğŸ“¦ Pythonä¾å­˜é–¢ä¿‚
```

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ãƒ«ãƒ¼ãƒ«

- **logs/**: ã™ã¹ã¦ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
- **results/**: å®Ÿé¨“çµæœã¯æ—¥æ™‚ä»˜ãã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ•´ç†
- **ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª**: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥é…ç½®ã—ãªã„

## ğŸ”§ å®Ÿè¡Œã®æµã‚Œ

1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™** (`prepare_data.py`)
   - éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
   - è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã¸ã®åˆ†å‰²ï¼ˆ70%/15%/15%ï¼‰
   - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ

2. **ãƒ¢ãƒ‡ãƒ«è¨“ç·´** (`train.py`)
   - MERT-v1-330Mãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
   - è¤‡æ•°ã‚¿ã‚¹ã‚¯ã®åŒæ™‚å­¦ç¿’ï¼ˆã‚¹ã‚³ã‚¢åˆ†é¡ã€æŠ€è¡“çš„ç‰¹å¾´ã®å›å¸°ï¼‰
   - æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ä¿å­˜

3. **ãƒ¢ãƒ‡ãƒ«è©•ä¾¡** (`evaluate.py`)
   - ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æ€§èƒ½è©•ä¾¡
   - ç²¾åº¦ã€F1ã‚¹ã‚³ã‚¢ã€å¹³å‡çµ¶å¯¾èª¤å·®ã®è¨ˆç®—
   - Attentionçµ±è¨ˆã®åˆ†æ

4. **å¯è¦–åŒ–ç”Ÿæˆ** (`visualize.py`)
   - è¤‡åˆå¯è¦–åŒ–ï¼ˆéŸ³å£°æ³¢å½¢ï¼‹ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‹Attentionï¼‰
   - Attentioné‡ã¿ã®ãƒ—ãƒ­ãƒƒãƒˆã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
   - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–HTMLãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦

### ãƒ‡ãƒ¼ã‚¿åé›†ã®æ¦‚è¦
- **æœ‰åŠ¹ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: 237ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ã544ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ï¼‰
- **è©•ä¾¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°**: 6,517ä»¶ã®å°‚é–€å®¶è©•ä¾¡

### ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ€ãƒªãƒ†ã‚£

#### 1. **éŸ³å£°ãƒ‡ãƒ¼ã‚¿** ğŸµ
- `sound_raw.wav`: æ¼”å¥éŸ³å£°ï¼ˆ16kHzã€ãƒ¢ãƒãƒ©ãƒ«ï¼‰

#### 2. **ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿** ğŸ¹
- `hackkey.csv`: 88éµç›¤ã®æŠ¼ä¸‹ãƒ‡ãƒ¼ã‚¿
  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: 1000Hzï¼ˆ1msè§£åƒåº¦ï¼‰
  - å€¤: ã‚­ãƒ¼æŠ¼ä¸‹æ·±åº¦ï¼ˆ0-127ï¼‰

#### 3. **è©•ä¾¡ã‚¹ã‚³ã‚¢** ğŸ“
- `score1`: ä¸»è¦è©•ä¾¡ï¼ˆ0-6ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
- ãƒ©ãƒ™ãƒ«ç”Ÿæˆ: score1 â‰¥ 4 ã‚’ã€Œè‰¯ã„æ¼”å¥ã€ã¨ã—ã¦äºŒå€¤åˆ†é¡

## ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### åŸºæœ¬è¨­å®š (`configs/config.yaml`)
- MERT-v1-330Mã‚’ä½¿ç”¨ã—ãŸæ¨™æº–è¨­å®š
- ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ï¼ˆã‚¹ã‚³ã‚¢åˆ†é¡ï¼‹æŠ€è¡“è©•ä¾¡ï¼‰
- 200ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´

## ğŸ“‹ é–‹ç™ºã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### âœ… æ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œ
- **æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£ã‚’å„ªå…ˆ** - æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå‰ã«æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®æ”¹è‰¯ã‚’æ¤œè¨
- **æ©Ÿèƒ½åˆ†å‰²** - è¤‡é›‘ã«ãªã£ãŸå ´åˆã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²
- **ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤** - å®Ÿé¨“ã‚„ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½¿ç”¨å¾Œã«å¿…ãšå‰Šé™¤
- **å¯èª­æ€§é‡è¦–** - ã‚³ãƒ¼ãƒ‰ã®æ•´ç†ã¨é©åˆ‡ãªã‚³ãƒ¡ãƒ³ãƒˆ

#### âŒ é¿ã‘ã‚‹ã¹ãå¯¾å¿œ
- **ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ** - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã§å¯¾å¿œå¯èƒ½ãªå ´åˆã®æ–°è¦ä½œæˆ
- **ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®æ”¾ç½®** - `test_*.py`, `debug_*.py`, `temp_*.txt`ãªã©ã®æ®‹å­˜
- **æ§‹é€ ã®è¤‡é›‘åŒ–** - å¿…è¦ä»¥ä¸Šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéšå±¤ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ™ è¬è¾

- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: ãƒ”ã‚¢ãƒæ¼”å¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æä¾›å…ƒ
- **ãƒ¢ãƒ‡ãƒ«**: Hugging Face Transformersã«åŸºã¥ãå®Ÿè£…
- **ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: è¨€èªå­¦ç¿’å‘ã‘DDSupportã®æ–¹æ³•è«–

---

**Profy** - *AIã®åŠ›ã§ãƒ”ã‚¢ãƒæ¼”å¥ã‚’å‘ä¸Šã•ã›ã‚‹* ğŸ¹âœ¨
# Profy: Multimodal Piano Performance Analysis and Highlights

This repository contains the Profy system for learning expertiseâ€‘dependent differences from synchronized keyâ€‘sensor (1 kHz) and audio streams and rendering localized, scoreâ€‘synchronized highlights that guide practice.

## Model Overview

- Backbone: `UnifiedAttentionModel` (see `src/models/unified_attention_model.py`)
  - Sensor encoder: 1D CNN with multiâ€‘kernel fusion (3/5/9), maxâ€‘pooling, projection to hidden.
  - Audio encoder: 2D CNN over timeâ€“frequency with a frequencyâ€‘attention head that learns perâ€‘band weights before temporal pooling (prevents losing informative bands).
  - Crossâ€‘modal attention: bidirectional multiâ€‘head attention (sensorâ†”audio) with keyâ€‘padding masks; parametric resamplers align streams to a common temporal grid.
  - Fusion: qualityâ€‘aware gating (uses audio quality vector: nonâ€‘silence rate, spectral flatness, loudness) and temperature scaling; outputs perâ€‘sample modality weights and entropy.
  - Temporal modeling: BiLSTM â†’ temporal attention (with masking) and an evidence head (sigmoid per frame) + a global classifier (expert vs amateur).
  - Masking: audio/sensor masks propagate to attention/evidence to suppress silent/invalid frames.

## Training Losses

- Classification: crossâ€‘entropy on global logits.
- Evidence learning (weak supervision):
  - MIL (Noisyâ€‘OR) over frames to match clipâ€‘level label.
  - Soft Topâ€‘k pooling (mean of top `k = frac * L` frames) blended with Noisyâ€‘OR (`alpha`), stabilizing against spiky singleâ€‘frame peaks.
  - L1 sparsity on evidence to encourage localized peaks.
- Attention energy regularization (optional): discourages attention aligning purely with frame energy (approx. meanâ€‘abs over 128 audio features).

Defaults (debug/full): `--lambda-mil 0.5`, `--lambda-evidence-l1 1e-3`, `--mil-topk-frac 0.1`, `--mil-blend-alpha 0.5`, `--lambda-attention-energy 0.0` (set to small positive e.g. `0.002` to enable).

## Reproducible Runs

### 1) Environment

- Python 3.10 recommended. Activate venv and install web UI deps as needed:
  - `python -m venv .venv && source .venv/bin/activate`
  - `pip install -r webapp/requirements.txt` (only if running the local web app)

### 2) Endâ€‘toâ€‘End Experiment (Debug)

Runs a quick pipeline (â‰ˆ60 samples) to verify everything:

```
./scripts/run.sh --debug --lambda-attention-energy 0.002
```

Outputs under `profy/results/experiment_YYYYMMDD_HHMMSS/`:
- `models/`: checkpoints (`sensor-_fold*.pth`, `audio-_fold*.pth`)
- `logs/`: training logs
- `figures/`: summary plots

### 3) Full Experiment (Save Checkpoints)

```
./scripts/run.sh --save-checkpoints --lambda-attention-energy 0.002
```

This runs 3â€‘fold GroupKFold with all 6,476 samples and stores models for alignment analysis.

### 4) Expertâ€‘Alignment Evaluation (Agreement)

Evaluate highlight agreement with expert annotations (Topâ€‘20 set, per annotator and consensus). Use the latest experiment directory and the provided web annotation folder:

```
./scripts/run_expert_alignment.sh \
  results/experiment_YYYYMMDD_HHMMSS \
  ../piano-performance-marker/web_evaluations \
  ../piano-performance-marker/public/audio \
  ../piano-performance-marker/public/audio/manifest_top20.json \
  profy/data \
  --consensus mean --consensus-thr 0.5 --min-evaluators 3
```

Outputs under `results/expert_alignment_YYYYMMDD_HHMMSS/`:
- `summary.json`: perâ€‘annotator aggregate metrics
- `summary_consensus.json`: consensus metrics (vote proportion â‰¥ threshold)
- `per_audio_metrics.csv`, `per_audio_metrics_consensus.csv`
- `figures/overlay_*`: overlays for individual raters and consensus

## Tips and Notes

- Debug vs Full: debug uses ~60 samples for fast verification; full uses the entire corpus.
- Gating: model logs mean modality weights and entropy; decisionâ€‘level fusion (PoE) is used for robust metrics, while midâ€‘level attention/evidence feeds the UI and alignment evaluation.
- Reproducibility: `config_manifest.json` is saved under each experiment directory; alignment scripts read `results/â€¦/models` and tune only lightweight postâ€‘processing (smooth/lag/power).
