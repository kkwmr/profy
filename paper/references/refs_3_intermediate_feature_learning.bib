% Intermediate level - feature learning
@inproceedings{han2014,
  title     = {Hierarchical Approach to Detect Common Mistakes of Beginner Flute Players},
  author    = {Yoonchang Han and Kyogu Lee},
  year      = {2014},
  url       = {http://www.terasoft.com.tw/conf/ismir2014/proceedings/T015_165_Paper.pdf},
  researchr = {https://researchr.org/publication/HanL14-12},
  cites     = {0},
  citedby   = {0},
  pages     = {77-82},
  booktitle = {Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014, Taipei, Taiwan, October 27-31, 2014},
  editor    = {Hsin-Min Wang and Yi-Hsuan Yang and Jin Ha Lee}
}
@inproceedings{wu2018,
  author    = {Wu, Chih-Wei and Lerch, Alexander},
  booktitle = {2018 IEEE 12th International Conference on Semantic Computing (ICSC)},
  title     = {Learned Features for the Assessment of Percussive Music Performances},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {93-99},
  doi       = {10.1109/ICSC.2018.00022}
}

@article{pati2018,
  author         = {Pati, Kumar Ashis and Gururani, Siddharth and Lerch, Alexander},
  title          = {Assessment of Student Music Performances Using Deep Neural Networks},
  journal        = {Applied Sciences},
  volume         = {8},
  year           = {2018},
  number         = {4},
  article-number = {507},
  url            = {https://www.mdpi.com/2076-3417/8/4/507},
  issn           = {2076-3417},
  abstract       = {Music performance assessment is a highly subjective task often relying on experts to gauge both the technical and aesthetic aspects of the performance from the audio signal. This article explores the task of building computational models for music performance assessment, i.e., analyzing an audio recording of a performance and rating it along several criteria such as musicality, note accuracy, etc. Much of the earlier work in this area has been centered around using hand-crafted features intended to capture relevant aspects of a performance. However, such features are based on our limited understanding of music perception and may not be optimal. In this article, we propose using Deep Neural Networks (DNNs) for the task and compare their performance against a baseline model using standard and hand-crafted features. We show that, using input representations at different levels of abstraction, DNNs can outperform the baseline models across all assessment criteria. In addition, we use model analysis techniques to further explain the model predictions in an attempt to gain useful insights into the assessment process. The results demonstrate the potential of using supervised feature learning techniques to better characterize music performances.},
  doi            = {10.3390/app8040507}
}
@inproceedings{sonospace,
  author    = {Kimura, Naoki and Shiro, Keisuke and Takakura, Yota and Nakamura, Hiromi and Rekimoto, Jun},
  title     = {SonoSpace: Visual Feedback of Timbre with Unsupervised Learning},
  year      = {2020},
  isbn      = {9781450379885},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi-org.utokyo.idm.oclc.org/10.1145/3394171.3413542},
  doi       = {10.1145/3394171.3413542},
  abstract  = {One of the most difficult things in practicing musical instruments is improving timbre. Unlike pitch and rhythm, timbre is a high-dimensional and sensuous concept, and learners cannot evaluate their timbre by themselves. To efficiently improve their timbre control, learners generally need a teacher to provide feedback about timbre. However, hiring teachers is often expensive and sometimes difficult. Our goal is to develop a low-cost learning system that substitutes the teacher. We found that a variational autoencoder (VAE), which is an unsupervised neural network model, provides a 2-dimensional user-friendly mapping of timbre. Our system, SonoSpace, maps the learner's timbre into a 2D latent space extracted from an advanced player's performance. Seeing this 2D latent space, the learner can visually grasp the relative distance between their timbre and that of the advanced player. Although our system was evaluated mainly with an alto saxophone, SonoSpace could also be applied to other instruments, such as trumpets, flutes, and drums.},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  pages     = {367â€“374},
  numpages  = {8},
  keywords  = {machine learning, music performance analysis, timbre analysis, music practice, deep neural network, variational auto encoder},
  location  = {Seattle, WA, USA},
  series    = {MM '20}
}
@inproceedings{seshadri2021,
  title     = {Improving Music Performance Assessment With Contrastive Learning},
  author    = {Pavan Seshadri and Alexander Lerch 0001},
  year      = {2021},
  url       = {https://archives.ismir.net/ismir2021/paper/000079.pdf},
  researchr = {https://researchr.org/publication/Seshadri021},
  cites     = {0},
  citedby   = {0},
  pages     = {634-641},
  booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR 2021, Online, November 7-12, 2021},
  editor    = {Jin Ha Lee 0001 and Alexander Lerch 0001 and Zhiyao Duan and Juhan Nam and Preeti Rao and Peter van Kranenburg and Ajay Srinivasamurthy},
  isbn      = {978-1-7327299-0-2}
}