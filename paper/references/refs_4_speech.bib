% Speech domain
@inproceedings{mirsamadi2017,
  author    = {Mirsamadi, Seyedmahdad and Barsoum, Emad and Zhang, Cha},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Automatic speech emotion recognition using recurrent neural networks with local attention},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {2227-2231},
  doi       = {10.1109/ICASSP.2017.7952552}
}
@inproceedings{conneau2020,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}
@inproceedings{fan2021,
  author    = {Zhiyun Fan and Meng Li and Shiyu Zhou and Bo Xu},
  title     = {{Exploring wav2vec 2.0 on Speaker Verification and Language Identification}},
  year      = 2021,
  booktitle = {Proc. Interspeech 2021},
  pages     = {1509--1513},
  doi       = {10.21437/Interspeech.2021-1280}
}
@misc{wang2021,
  doi = {10.48550/ARXIV.2111.02735},
  
  url = {https://arxiv.org/abs/2111.02735},
  
  author = {Wang, Yingzhi and Boumadane, Abdelmoumene and Heba, Abdelwahab},
  
  keywords = {Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE), Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{kawamura2021uist,
  author    = {Kawamura, Kazuki and Rekimoto, Jun},
  title     = {A Language Acquisition Support System That Presents Differences and Distances from Model Speech},
  year      = {2021},
  isbn      = {9781450386555},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3474349.3480225},
  doi       = {10.1145/3474349.3480225},
  abstract  = {It is difficult for language learners to know whether they are speaking well and, if not, to know where their speech differs from that of native speakers and how much their speech differs from that of native speakers. Therefore, we propose a novel language learning system that solves these problems. The system uses self-supervised learning to determine whether the user’s speech is good or not. The system also shows where the user’s speech differs from the native speaker’s speech by highlighting the corresponding places on the user’s speech waveform. It also represents the learner’s speech and the native speaker’s speech as points on a two-dimensional coordinate to show the user how far apart they are. We expect that the learner’s speech will gradually become better by repeatedly modifying the speech to eliminate these differences and bring the distance closer.},
  booktitle = {The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology},
  pages     = {44–46},
  numpages  = {3},
  keywords  = {Attention Models, Neural Networks, Self-supervised Learning, Language Acquisition},
  location  = {Virtual Event, USA},
  series    = {UIST '21}
}
@conference{ddsupport,
  title     = {DDSupport: Language Learning Support System that Displays Differences and Distances from Model Speech},
  author    = {Kawamura, Kazuki and Rekimoto, Jun},
  booktitle = {21st IEEE International Conference on Machine Learning and Applications (ICMLA)},
  month     = {Dec},
  year      = {2022}
}