%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,review,anonymous]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[AHs '23]{AHs ’23}{March  12-14, 2023}{Glasgow, United Kingdom}
\acmBooktitle{AHs ’23, March  12-14, 2023, Glasgow, United Kingdom}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Automated Analysis of Musical Expression in Piano Performance}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Fujiki Nakamura}
\affiliation{%
  \institution{The University of Tokyo}
  \city{Tokyo}
  \country{Japan}}
\email{fujiki-nakamura@g.ecc.u-tokyo.ac.jp}

\author{Shinichi Furuya}
\affiliation{%
  \institution{Sony Computer Science Laboratories Kyoto}
  \city{Tokyo}
  \country{Japan}
}
\email{furuya@csl.sony.co.jp}

\author{Jun Rekimoto}
\affiliation{%
  \institution{The University of Tokyo}
  \institution{Sony Computer Science Laboratories Kyoto}
  \city{Tokyo}
  \country{Japan}
}
\email{rekimoto@acm.org}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Nakamura et al.}

\renewcommand{\figurename}{Figure.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  It can be difficult for learners to independently assess and develop musical skills at the intermediate and advanced levels.
  This is why many people turn to music coaches for help, but finding a suitable coach can be challenging. 
  Some coaches may not be available when needed or may not possess the appropriate teaching style. 
  While there are computer-aided tools for analyzing and improving basic musical skills, there is a lack of support for developing the intermediate and advanced skills. 
  In addition, there has been little research on the automatic analysis of musical expression.
  To address this gap, we propose a Transformer-based model for automatically assessing the brightness of piano performance, which is one aspect of musical expression. 
  We created a dataset of recordings from 20 professional pianists playing the same phrases with either a bright or dark expression. 
  We trained the model on this dataset, achieving an accuracy of approximately 80\%. 
  The model also provides visual feedback by highlighting important portions of the performance. 
  By using this model, learners can better understand the components of musical skill and can work on improving their skills independently. 
  We believe that this model has the potential to accelerate the acquisition of intermediate and advanced musical skills.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003121.10003124.10011751</concept_id>
       <concept_desc>Human-centered computing~Collaborative interaction</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003145.10003146</concept_id>
       <concept_desc>Human-centered computing~Visualization techniques</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Collaborative interaction}
\ccsdesc[500]{Human-centered computing~Visualization techniques}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{automatic musical performance analysis, musical skill acquisition, musical expression, deep learning, Transformer}

\begin{teaserfigure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/teaser.v1.8.png}
    \caption{
        An overview of our model. (1) The player performs with either a bright or dark musical expression. (2) The model pre-trained on a large-scale audio dataset and fine-tuned to the musical performance analyzes expression of the performance. (3) The model highlights the important aspects for the analysis. (4) Finally, it provides the player with feedback in a visual format.
    }
    \label{teaser}
\end{teaserfigure}

%\received{6 December 2022}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% from beginner to intermediate or higher
It is difficult for music players to progress from beginner to intermediate and advanced levels. 
When they first begin playing an instrument, they typically begin at the beginner's level and learn the basics.
To become a skilled player, it is necessary for them to continue practicing and improving upon these fundamentals.

% Skills at beginner's level
The basics of playing an instrument are generally easier to learn than more advanced techniques.
This is because the basic skills often consist of factors that are easy to grasp, such as accuracy of loudness, pitch, timing, and rhythm. 
These skills can be objectively measured using tools such as a tuner for pitch and a metronome for rhythm. 
By using these objective metrics, music players can practice and improve upon the fundamentals of playing their instrument. 
Through iterative trial and error, they can achieve the goals of their practice and make progress. 
Consequently, most music players are able to independently learn the basics of playing an instrument at the beginner's level.

% skills at intermediate or higher level
% They are subjective and not objectively measurable.
% So hard to understand and to acquire.
However, many music players struggle to improve beyond beginner-level skills and reach the intermediate and advanced levels. 
One potential reason for this is the difficulty in defining and measuring more advanced musical skills. 
Unlike basic skills to control loudness, pitch, and rhythm, intermediate and advanced skills to control timbre, articulation, and musical expression are more complex and difficult to understand, particularly for beginners. 
These skills are often understood in broad, vague terms, thereby making it difficult to define them rigorously and understand them objectively. 
Consequently, many music players may struggle to improve upon these more advanced skills.

% Need for music coaches and their problem
Therefore, they often rely on skilled coaches or mentors to help them improve their skills. 
These coaches have more experience and advanced skills than learners and can provide valuable feedback on learners' performance. 
However, it can be difficult to find and hire such coaches, particularly if they are not located nearby or are expensive. 
Additionally, the coaching methods used may vary depending on the coach and may be based on their personal experiences, thereby making it difficult for learners to benefit from the coaching. 
This can be frustrating for learners and hinder their progress in acquiring new musical skills.

% Existing approach for intermediate musical skill analysis
To address this problem, certain researches have developed methods for analyzing an intermediate musical skill. One such method is SonoSpace \cite{SonoSpace}, which analyzes timbre control using a Variational AutoEncoder (VAE).
This method learns the discriminative features of timbre in saxophone recordings. 
It also projects them onto a two-dimensional space, providing visual feedback for learners to use in ascertaining the accuracy of their own timbre.

% musical expression
Despite the success of timbre analysis, other intermediate or advanced musical skills have not yet been analyzed in depth.
One such skill is musical expression.
The musical expression is a way in which a musician performs a piece of music and the kind of effect this performance has on the emotions of the listeners, even if they have heard the same performance before. 
This is because musical expression --- which includes elements such as phrasing, dynamics, and brightness --- plays a significant role in shaping the overall listening experience. 
Moreover, other factors such as timbre and articulation may also contribute to musical performance, but these factors may not have as great an impact on the listener as musical expression.
% In order to develop these expressive musical skills, it is insufficient just to emulate or refer to some role model musical performance by measuring how close the developing skill is to the role model performance.
% One of the reason for it is that the musical expression might depend on a musical context, resulting a need for more holistic analysis of the expression, and making it challenging for learners to master on their own.
In order to develop expressive musical skills, simply emulating or referencing the performance of a role model is not sufficient.
This is because musical expression depends on the musical context, thereby requiring a more holistic analysis of the expression. 
This can make it challenging for learners to master expressive skills independently. 
People require a more comprehensive approach to analyze and learn music. 
% to help themselves develop the skills.

% proposal
Therefore, in this study, we propose an automated analytical model that assesses and visualizes this skill using an audio recognition model based on Transformer-based Deep Learning technique. 
We focus specifically on the skill of controlling the expression of brightness in piano performance, which involves changing the style of a performance to create a feeling of brightness. 
When certain skillful musicians play a song with a bright expression, the audience feels the brightness in the performance; however if the musicians play with a dark expression, the audience feels the darkness.
The proposed model learns to recognize the musical expression from a small number of audio recordings of expressive piano performances by 15 professional pianists.
This model achieves an accuracy of approximately 80\% in classifying the expression. 
We also show how our model can provide explainable assessments of musical skill by visualizing the attention mechanism of the model. 
This enables learners to see which parts of their performance are most important for the skill being assessed, thereby providing concrete insights into an otherwise vague and difficult-to-perceive aspect of their performance.
By using our proposed model, learners can receive valuable feedback on their expressive performance abilities and improve their skills at their own pace.


\section{Related work}
There have been studies and commercial applications relate to the basic analysis of musical performance, such as SmartMusic \cite{SmartMus76:online} and Yousician \cite{Yousicia62:online}. 
These tools assess the quality of musical performance in terms of pitch and timing, and provide users with real-time feedback. 
In general, the basic analysis often focuses on loudness, pitch, and rhythm to assess the quality of the performance.
These factors can be objectively quantified using metrics and tools such as tuners, metronomes, and the above mentioned tools. 

In addition to basic analysis, several studies have focused on more advanced techniques for assessing the subjective quality of musical performance. 
These approaches often involve the creation of audio descriptors or the extraction of handcrafted features from the performance, which are then used to train machine learning models to predict the quality ratings of human experts \cite{nakano2006automatic,PDFAreal4:online, vidwans2017objective}. 
Other approaches use more generic audio features, such as Mel-spectrograms, to train deep neural networks in an end-to-end manner \cite{app8040507}. 
While these methods can be effective, they often rely on the knowledge and characteristics of individual musical instruments, and may not capture all aspects of musical performance. 
Consequently, they may not attain the level of accuracy achieved by human experts, thereby leaving room for further advancement in the analysis of musical performance.

Further with regard to analysis on intermediate and advanced skills, like control over the instrument's timber and articulation, SonoSpace uses a VAE to analyze the timbre of saxophone \cite{SonoSpace}. 
SonoSpace learns the quality of timbre in an unsupervised manner, without the need for human annotations. 
Furthermore, SonoSpace provides visual feedback by displaying a two-dimensional mapping of the timbre, thereby making its subjective quality easily understandable. 
However, timbre is only one aspect of a musical performance, and other subjective aspects such as musical expression play a crucial role in the overall quality of the performance. 
These aspects can often constitute a longer portion of the performance and significantly affect how people feel during the performance.
Therefore, we need new methodologies to analyze the subjective quality of musical expression.

In the field of speech analysis, which is related to musical analysis, there are several approaches to analyze emotions and languages \cite{EmotionRNN, conneau-etal-2020-unsupervised}. 
These approaches do not rely heavily on domain-specific features; instead, these approaches involves a holistic analysis of speech using raw audio waveform as inputs to the model. 
Additionally, Mirsamadi et al. \cite{EmotionRNN} indicate attention mechanisms play an important role in speech analysis by the visualization of important temporal components of emotion in speech. Kawamura et al. \cite{KawamuraUist21, ddsupport} propose a speech analysis method for language learning and provides a support tool to help language learners improve their speaking skills in a foreign language. 
The tool provides visual feedback to learners by highlighting the differences between their speech and that of native speakers. 
This visual feedback is generated by overlaying the attention of their speech analysis model on the speech waveform over time. 

Because their methodology can be domain-independent, it can be applied to other fields, such as music, and can serve as an inspiration for the analysis of the subjective quality of musical expression in performance. 
% To the best of our knowledge, our method is the first work on the holistic approach of the musical analysis that do not rely on specific techniques such as hand-engineered features of some aspects of the musical performance and is distinguishable from an emulating or referencing approach to some role model performance.
% And it shows how we can incorporate advanced methodologies in other areas to an artistic area of music.
To the best of our knowledge, our method is the first to adopt a holistic approach to musical analysis that considers a performance as a whole.
The approach does not rely on hand-engineered features or specific techniques, and is also distinguishable from emulating or referencing the performance of a role model. 
Additionally, our work demonstrates how advanced methodologies from other fields can be applied to the artistic domain of music.


\section{Method}
% introduce wav2vec 2.0
By drawing an analogy to the expressions used in speech, we use the techniques developed for speech recognition to analyze the expression in musical performance.
In particular, we employ wav2vec 2.0 \cite{wav2vec2}, which is a large pre-trained model trained on the Librispeech corpus \cite{Librispeech}. 
Because wav2vec 2.0 has already been trained on a large corpus, we can expect that it can be fine-tuned for the analysis of a musical performance using a relatively small dataset. 
% This approach takes advantage of the advances in speech recognition technology to provide a new way of analyzing musical expression.

% attention
The wav2vec 2.0 model is based on the Transformer architecture, which includes an attention mechanism. 
This mechanism allows the model to assign different weights to different parts of its input data, thereby allowing it to focus on the most important aspects of the input. 
This is particularly useful when fine-tuning the model for a specific downstream task, as it enables the model to focus on the most relevant parts of the input data to solve the given task. 
% In this way, the attention mechanism allows the model to learn more effectively during both pre-training and fine-tuning.

% visualization of attention
One benefit of using a model with an attention mechanism, such as wav2vec 2.0, is that we can inspect and visualize the attention weights to identify the portions of the input that the model considers most important for solving the given task. 
In this study, we use this approach to identify the portions of a piano performance that are most important for the expression of brightness. 
This enables us to better understand and distinguish between different musical expressions, which can be difficult to grasp intuitively. 
By visualizing the attention weights, we can gain a more concrete understanding of the differences between various musical expressions.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/wav2vec2.0_v1.2.png}
  \caption{Overview of the development of our model. 
  (Left) Because wav2vec 2.0 can learn useful features without any supervision, it can be initially trained on a large audio corpus, such as speech. 
  Through self-supervised pre-training, the model develops representations of input data that can be useful in later stages of development. 
  (Right) During fine-tuning on a downstream task, like music performance analysis, the pre-trained model can learn the task with a small dataset.
  This is because it transfers the representations learned in pre-training to similar tasks.}
  \Description{}
  \label{model_overview}
\end{figure}


\section{Experiments}
\subsection{Expressive Piano Performance Dataset}
% about pianists
In order to conduct our study, we collected 159 recordings of piano performances from 20 professional pianists, including several winners of international and Japanese competitions. 
We asked each of the pianists to play four different songs with a specified musical style or expression. 
% about songs
The four songs include a Hanon exercise and three different excerpts from two Chopin Etude (one from Etude Op. 25 No. 2 and the other two from Etude Op. 10 No. 4). 
We selected these songs because they are commonly used for piano practice. 
We asked the pianists to play each song with either a bright or dark expression. 
One performance was excluded from the dataset due to errors in the performance, thereby resulting in a dataset of 159 recordings.
A few of the recordings are available on \href{https://drive.google.com/drive/folders/1xp3xK3R8Lk2Ju28Suqh3HeLBHwiNSn1E}{this https URL}.

% recording specification
All recordings of the same song are of the same duration. 
For example, the recordings of the Hanon exercise are approximately 4 second long (with 2 bars), while the recordings of Chopin Etude Op. 25 No. 2 are approximately 14 second long (with 4 bars). 
Similarly, the recordings of the two excerpts from Chopin Etude Op. 10 No. 4 are approximately 10 second long (with 4 bars). 
This ensures that the performances of each song do not much differ in terms of duration, but only in terms of their expression (bright or dark). 
% This enables us to compare the performances of the same song across different expressions, without the confounding factor of differences in duration.

% about brightness expression. what is brightness?
The brightness and darkness of a musical performance is not determined by the composition of the song itself, but rather by how it is played or expressed by the pianist. 
This aspect of musical performance is often associated with the emotional expression that people experience while listening to a piano performance. 
Being able to create such an expression is considered a skill at the intermediate and advanced levels of piano performance. 

% Others: recording, pre-processing, etc.
The recordings of the piano performances were made at a sampling rate of 44.1 kHz. 
However, the model that we used for our analysis, wav2vec 2.0, was pre-trained at a sampling rate of 16 kHz. 
Therefore, we resampled the recordings to 16 kHz in the pre-processing stage before training our model. 
During training, we further truncated the audio data of the performances to a duration of 10 seconds, thereby resulting in 160,000 samples or data points of the audio waveform (16 kHz $\times$ 10 seconds). 


\subsection{Modeling}
Our model for analyzing piano performance is based on a base model size of wav2vec 2.0. 
It consists of a feature extraction module and a transformer module. 
The feature extraction module uses 7 layers of convolutional neural networks to extract features from the audio waveform input. 
The transformer module consists of 12 layers of transformer blocks, each of which has 12 attention heads. 
These attention heads enable the model to focus on different parts of the input and to understand better the input and make accurate predictions on it.
% Consequently, the model learns contextualized representations that enable it to understand the input and make accurate predictions.
% The model was implemented using the HuggingFace library and its APIs ([]).

We used a 4-fold cross-validation approach to devide our piano performance dataset into training and validation sets. 
This involved selecting 5 pianists and using all their performances as the validation set, while the performances of the remaining pianists were used as the training set. 
We repeated this process four times, thereby resulting in four different models that were trained to analyze piano performance. 
% We report the average performance of these models, as well as the performance of each individual model. 

We began training our models using the initial weights of the base model.  % provided by HuggingFace Hub.
Thereafter, we fine-tuned them on our piano performance dataset for 100 epochs. 
The batch size was set to 32 and the learning rate was set to 3e-6, with linear decay throughout the training process. 
The models were trained on a GPU of NVIDIA V100. 


\section{Results}
\subsection{Evaluation of the automated analysis of musical expression}
Since we collected an equal number of performances for each expression (bright and dark), we were able to evaluate the performance of our model using the metric of accuracy. 

The average accuracy of our model across the four folds of cross-validation was 79.23\%. 
The results for each individual fold are presented in Table \ref{accuracy}. 
This is significantly higher than the by chance accuracy of 50\%.
This result indicates that the model is able to accurately judge whether a performance is expressed in a bright or dark manner. 
This demonstrates the effectiveness of our approach in analyzing the expression of piano performances.

\begin{table*}[h!]
  \caption{Accuracy of the model's performance in skill assessment}
  %\label{tab:commands}
  \begin{tabular}{c|rrrr|r}
    \toprule
    Set & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Average\\
    \midrule
    Training & 0.9495 & 0.9411 & 0.7416 & 0.8151 & 0.8618\\
    Validation & 0.85 & 0.925 & 0.769 & 0.625 & 0.7923
    \bottomrule
    \label{accuracy}
  \end{tabular}
\end{table*}
% the original data
% 0.9495	0.85
% 0.9411	0.925
% 0.7416	0.7692
% 0.8151	0.625
% 0.8618   0.7923


\subsection{Visualization of Attention}
The above result shows that our model is able to accurately assess the expression of piano performances.
This would make it a useful tool for providing feedback to pianists. 
However, in order for the model to provide effective coaching, it must be able to explain its assessment in a way that is understandable to human users.

% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.5\linewidth]{figures/viz_attn.hanon.perf_bright.pred_bright.png}
%   \caption{Visualization of attention on the Hanon performed with bright expression.}
%   \Description{}
%   \label{attn_bright}
% \end{figure}

% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.5\linewidth]{figures/viz_attn.hanon.perf_dark.pred_dark.png}
%   \caption{Visualization of attention on the Hanon performed with dark expression.}
%   \Description{}
%   \label{attn_dark}
% \end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/hanon_score_and_viz_v1.1.png}
  \caption{Visualization of attention on the performance of Hanon. (a) shows the attention highlight on the performance with a bright expression, and (b) shows the attention highlight on the performance with a dark expression.}
  \Description{}
  \label{hanon_attn}
\end{figure}

We addressed this issue by visualizing the model's attention on the input audio waveform, as illustrated in Figure \ref{hanon_attn}. 
The figure highlights in red the portions of the waveform that the model attended to, thereby enabling us to see which portions of the performance the model is focusing on for its assessment. 
Figure \ref{hanon_attn} (a) depicts the model's attention when assessing a bright performance, while Figure \ref{hanon_attn} (b) depicts its attention when assessing a dark performance. 
By analyzing these attention maps, we can gain insight into the factors that the model considers important for the skill assessment.

Comparing the attention visualizations for bright and dark performances, we find that the model attends to different parts of the input audio waveform in each case. 
This indicates that the model has learned to differentiate between these two expressions.
Furthermore it is able to provide an explanation for how it makes the distinction. 
By attending to different parts of the input data, the model is able to identify the features that are most important for identifying the difference between bright and dark expressions. 
% This ability to provide interpretable explanations for its skill assessment makes the model a more effective coach for piano performers.

Figure \ref{chopin-25-2_attn} illustrates additional visualizations of the attention.
This includes a case where the model's skill assessment differs from the intended expression of the performance (Figure \ref{chopin-25-2_attn} (b)). 
In both figures, the portions of the waveform that the model attended to are highlighted in red again.  
%, thereby enabling us to see which aspects of the performance the model is focusing on.

% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.5\linewidth]{figures/viz_attn.perf_dark.pred_dark.png}
%   \caption{Visualization of attention on Chopin Etude performed with dark expression AND evaluated as dark.}
%   \Description{}
%   \label{attn_dark_dark}
% \end{figure}

% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.5\linewidth]{figures/viz_attn.perf_dark.pred_bright.png}
%   \caption{Visualization of attention on Chopin Etude performed with dark expression BUT evaluated as bright.}
%   \Description{}
%   \label{attn_dark_bright}
% \end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/chopin-25-2_score_and_viz_v1.1.png}
  \caption{Visualization of attention on the performance of Chopin Etude Op. 25 No. 2. (a) shows the attention highlight on the performance with a dark expression AND assessed as dark, whereas (b) shows the attention highlight on the performance with a dark expression BUT assessed as bright.}
  \Description{}
  \label{chopin-25-2_attn}
\end{figure}

Figure \ref{chopin-25-2_attn} (a) illustrates the attention when assessing a performance by a pianist who is highly skilled at expressive performance. 
In this case, the pianist performed with a dark expression, and the model accurately classified the performance as dark. 
In contrast, Figure \ref{chopin-25-2_attn} (b) depicts the attention when assessing a performance by a pianist who is not as skilled at expressive performance. 
In this case, the pianist performed with a dark expression, but the model classified the performance as bright. 

Comparing the figures, we find again that the model attends to different portions of the input audio waveform in each case. 
By using the model as a reference, we can identify which parts of the waveform are responsible for the opposite assessment in Figure \ref{chopin-25-2_attn} (b). 
For example, at approximately 12 seconds, we find that the model is attending to a part of the waveform that causes it to provide the opposite assessment of the performance. 
This indicates that the model can provide valuable feedback to pianists by identifying the portion of their performance that may be hindering the accurate expression of brightness.


\section{Discussion}
Based on the results of the model's performance regarding the skill assessment in terms of musical expression, it accurately assesses the skill and determine whether a performance is being expressed with a bright or dark expression.
This makes the model a potentially effective coach for expressive performance, particularly in cases where its assessment disagrees with that of the performer's, as depicted in Figure \ref{chopin-25-2_attn} (b).

In such a case, the player who performed the piece is not particularly skilled at the expressive performance. 
Consequently, their expression may have been less effective than desired. 
However, the coaching model is able to accurately assess the performance.
Therefore, by examining the visualization of its attention, the performer can identify which portions of the performance were deemed important and can potentially identify why their expression may not have been as effective as intended. 

Suppose the piano player is one of who wishes to improve upon their skills of expression in their performance. 
With the coaching model's skill assessment, they can determine how well they are playing a piece on their own and identify areas for improvement. 
If the model identifies certain portions of their performance as incorrect, the player can use the highlighted portions of the performance to understand where they can improve. 
By working on reducing or eliminating the highlighted portions, the player can improve their performance and develop the expressive skills they desire. 
As the highlighted portions disappear and the coaching model assesses the performance as good enough, the player will know that they have successfully learned and improved their skills. 
Overall, the coach model's automated analysis of the musical skill and visualization can provide a helpful and efficient process for the acquisition of musical skill.

Thus, we presented a basic and simplistic method for providing feedback, but we can explore ways to improve the visual feedback.
One potential means to improve the feedback is to highlight the musical score itself. 
This is a familiar format for musicians, as they use scores to learn and perform music. 
By incorporating feedback information directly into the score, we can provide a more intuitive and effective means for musicians to receive feedback on their performances. 
In the first step toward this goal, we plan to take the alignment of the score and audio recordings to provide score-based highlights, as we can record performances in a MIDI-like format.
This will provide a more visually compelling and easily understandable feedback experience for musicians.


\section{Conclusion}
In this paper, we proposed an automated analytical model for musical expression that assesses a musical skill in piano performance at intermediate and advanced levels.
Additionally, the model provides feedback by visualizing the basis of the skill assessment.
The skill involves performing songs with ether a bright or dark expression, and the model assesses the skill with an accuracy of approximately 80\%.
The model visualizes the important portions of the performances for skill assessment and reveals differences in musical expressions, which makes them visually understandable. We expect that the analytical model can accelerate the process of acquiring musical skill by providing an accurate skill assessment and providing feedback through visualizing the components of musical expression.

% In the future research, we plan to verify how much helpful the feedback from system is, along side developing better ways to provide feedback including overlay onto the musical score.
% Also, it is hopeful to study in an extension of the system to other skills in piano performance like expression in clearness of performance, or applying the method to other instruments than piano like wind instruments.
In future research, we plan to evaluate the effectiveness of the feedback provided by our model, and explore ways to improve the feedback, such as incorporating it directly into the musical score. 
Additionally, we aim to extend the model to evaluate other skills in piano performance, such as clarity of performance, and to potentially apply the method to other instruments, such as wind instruments.


\section{Acknowledgments}
In Figure 1, the icons are made by Freepik from www.flaticon.com.


\bibliographystyle{ACM-Reference-Format}
\bibliography{sections/references}


\end{document}