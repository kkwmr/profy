Figure 1. Overview of musical performance analysis pipeline based on Wav2MusicAnalysis. (1) A pianist plays the piano, in particular with some musical expression (here with a bright expression). (2) The audio recording of the performance is fed into our proposed music performance analysis model without any audio or musical feature extraction. (3) The model is based on the Transformer architecture and it has attention mechanism that allows the model to identify the portions of the performance that is important for the analysis. (4) The model analyzes the performance and assesses it as a performance with dark expression, which disagrees with the pianist's intended expression. The model also highlight the significant portions of the performance that it deemed contribute to the analysis result. (5) The highlight is translated into musical score format by aligning the performance recording and its musical score. The pianist utilizes the analysis result and the highlight information on the musical score to review their performance. They also playback the performance with the guidance of the highlight.

Figure 2. A simplified hierarchy of musical skills. There are three levels from bottom to top, namely beginner, intermediate and advanced levels. There are also two categories of musical elements or components according to how much they consists of musical performances. The short components at beginner level include loudness, pitch and timing while the long components at the level include rhythms. The short components at intermediate level include timbre, articulation and so on. The long components at intermediate level include general musical skill and musical expression such as phrasing, dynamics and brightness. The components at advanced level include performance style and others. The components at beginner level are categorized as objective aspect of musical performances and easy to measure. The components at intermediate and advanced levels are categorized as subjective aspects and hard to measure.

Figure 3. Overview of the development of our model. During the pre-training, the model is trained on large scale speech corpus. The model learns in self-supervised way with contrastive loss function. It learns useful audio features by learning to extract latent features from raw audio waveforms and then to build contextualized features throught the Transformer layers. After the pre-training, the model is fine-tuned on small musical performance datasets with few experts' labels. Durting the fine-tuning, the contextualized features from performance recordings are averaged over time, and used to make the musical skill assessment.

Figure 4. Overview of the visualization of attention after musical skill fine-tuning. During the fine-tuning, the model assigns different attention weights to different portions of the peroformance. After the fine-tuning, when the model infers to make skill assessment of a musical performance, it also provides attention weights. The model highlights the portions of the performance that have high values of the attention weights. The highlight is considered to be a basis of the performance analysis.

Figure 5. Recording environment for our musical skill assessment experiments. Left shows the soundproof room where we recorded all of the performances. Right shows the piano used for all of the performance experiments.

Table 1. The description of musical skill dataset for each general musical skill group. It has five columns: Group, the description of each group, the number of participants in each group, the number of performances from each group, and the number of the peformances that human experts' identify as skillful for each group with its percentage.

Table 2. The list of the musical pieces for the musical skill assessment dataset. It has four columns: Piece, the details of each piece, the mean of the duration in time of the performances of each piece, and the standard deviation of the performances of each piece.

Table 3. The list of the musical pieces for the musical brightness expression assessment dataset. It has four columns: Song, the details of each piece that describes which bars are used from each piece, the mean of the duration in time of the performances of each piece, and the standard deviation of the performances of each piece.

Table 4. The results of the model's performance in skill assessment. The table describes the F1 scores for training and validation sets in each fold. Namely, the model achieves F1 score of 1.000 for the training set, and 0.643 for the validation set in fold 1. The model achieves F1 score of 0.983 for the training set, and 0.667 for the validation set in fold 2. The model achieves F1 score of 0.992 for the training set, and 0.667 for the validation set in fold 3. The model achieves F1 score of 1.000 for the training set, and 0.714 for the validation set in fold 4. Across the folds, the models achives the average F1 score of 0.994 and 0.673 on training and validation set, respectively.

Figure 6. Confusion matrix for human experts' and the model's assessment of musical skill. The matrix has "Experts' Labels" rows whose first row represents "Not skillful" and whose second row represents "Skillful". The matrix has two columns of "Predictions", the first column represents "Not skillful" and the second column represents "Skillful". The cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Not skillful" has value of 43 and the percentage of 26.71%, the cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Skillful" has value of 33 and the percentage of 20.50%, the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Not skillful" has value of 20 and the percentage of 12.42%, and the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Skillful" has value of 65 and the percentage of 40.37%.

Table 5. The results of the model's performance in brightness assessment. The table describes the F1 scores for training and validation sets in each fold. Namely, the model achieves F1 score of 0.955 for the training set, and 0.821 for the validation set in fold 1. The model achieves F1 score of 0.928 for the training set, and 0.800 for the validation set in fold 2. The model achieves F1 score of 0.919 for the training set, and 0.750 for the validation set in fold 3. The model achieves F1 score of 0.958 for the training set, and 0.781 for the validation set in fold 4. Across the folds, the models achives the average F1 score of 0.940 and 0.788 on training and validation set, respectively.

Figure 7. Confusion matrix for human experts' and the model's assessment of musical skill for each musical skill group. Each matrix has "Experts' Labels" rows whose first row represents "Not skillful" and whose second row represents "Skillful". Each matrix has two columns of "Predictions", the first column represents "Not skillful" and the second column represents "Skillful". For group A, The cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Not skillful" has value of 6 and the percentage of 14.29%, the cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Skillful" has value of 7 and the percentage of 16.67%, the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Not skillful" has value of 5 and the percentage of 11.90%, and the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Skillful" has value of 24 and the percentage of 57.14%. For group B, The cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Not skillful" has value of 5 and the percentage of 14.29%, the cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Skillful" has value of 3 and the percentage of 8.57%, the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Not skillful" has value of 8 and the percentage of 22.86%, and the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Skillful" has value of 19 and the percentage of 54.29%. For group C, The cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Not skillful" has value of 12 and the percentage of 28.57%, the cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Skillful" has value of 9 and the percentage of 21.43%, the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Not skillful" has value of 6 and the percentage of 14.29%, and the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Skillful" has value of 15 and the percentage of 35.71%. For group D, The cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Not skillful" has value of 20 and the percentage of 47.62%, the cell of "Experts' Labels" of "Not skillful" and "Predictions" of "Skillful" has value of 14 and the percentage of 33.33%, the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Not skillful" has value of 1 and the percentage of 2.38%, and the cell of "Experts' Labels" of "Skillful" and "Predictions" of "Skillful" has value of 7 and the percentage of 16.67%.

Figure 8. Confusion matrix for human experts' and the model's assessment of musical brightness expression. The matrix has "Experts' Labels" rows whose first row represents "Bright" and whose second row represents "Dark". The matrix has two columns of "Predictions", the first column represents "Bright" and the second column represents "Dark". The cell of "Experts' Labels" of "Birght" and "Predictions" of "Bright" has value of 45 and the percentage of 29.80%, the cell of "Experts' Labels" of "Bright" and "Predictions" of "Dark" has value of 14 and the percentage of 9.27%, the cell of "Experts' Labels" of "Dark" and "Predictions" of "Bright" has value of 18 and the percentage of 11.92%, and the cell of "Experts' Labels" of "Dark" and "Predictions" of "Dark" has value of 74 and the percentage of 49.01%.

Figure 9. Confusion matrix for human experts' and the model's assessment of musical brightness expression for each skill level of the brightness expression. Each matrix has "Experts' Labels" rows whose first row represents "Bright" and whose second row represents "Dark". Each matrix has two columns of "Predictions", the first column represents "Bright" and the second column represents "Dark". For the skill level of "Very good at brightness expression", the cell of "Experts' Labels" of "Birght" and "Predictions" of "Bright" has value of 6 and the percentage of 18.75%, the cell of "Experts' Labels" of "Bright" and "Predictions" of "Dark" has value of 5 and the percentage of 15.62%, the cell of "Experts' Labels" of "Dark" and "Predictions" of "Bright" has value of 3 and the percentage of 9.38%, and the cell of "Experts' Labels" of "Dark" and "Predictions" of "Dark" has value of 18 and the percentage of 56.25%. For the skill level of "Good at brightness expression", the cell of "Experts' Labels" of "Birght" and "Predictions" of "Bright" has value of 17 and the percentage of 43.59%, the cell of "Experts' Labels" of "Bright" and "Predictions" of "Dark" has value of 3 and the percentage of 7.69%, the cell of "Experts' Labels" of "Dark" and "Predictions" of "Bright" has value of 6 and the percentage of 15.38%, and the cell of "Experts' Labels" of "Dark" and "Predictions" of "Dark" has value of 13 and the percentage of 33.33%. For the skill level of "Not good at brightness expression", the cell of "Experts' Labels" of "Birght" and "Predictions" of "Bright" has value of 22 and the percentage of 27.50%, the cell of "Experts' Labels" of "Bright" and "Predictions" of "Dark" has value of 6 and the percentage of 7.50%, the cell of "Experts' Labels" of "Dark" and "Predictions" of "Bright" has value of 9 and the percentage of 11.25%, and the cell of "Experts' Labels" of "Dark" and "Predictions" of "Dark" has value of 43 and the percentage of 53.75%.

Figure 10. Visualizsation of attention on the performance of Hanon. (a) shows the attention pattern 
with red highlight on the performance with a bright expression. (b) shows the attention pattern with red highlight on the performance with a dark expression.

Figure 11. Visualizsation of attention on the performance of Hanon. (a) shows the attention pattern with red highlight on the performance in which the pianist intends to perform with a bright expression, but the model and the experts identify it as dark. (b) shows the attention pattern with red highlight on the performance in which the pianist intends to perform with a dark expression, and the model and experts identify it as dark. The attention shows a similar pattern for both (a) abnd (b) around the time from 1.0 to 1.5 seconds.

Figure 12. Musical score based user interface based on our method. Mainly it has two components, buttons and musical score display. One of the buttons showed in top left of the UI is named "Select a sample" button, by which music practitioners select a musical piece of their performance. By using pull down buttons named "Select a performance", they select a performance to analyze. For the analysis of musical brightness expression in particular, they select a performance with either bright or dark expression from the pull down buttons. There is "AI Analysis" field, which has "Highlight" button. By clicking it after selecting a performance, the model's analysis result shows up in the field, and highlight in red are made on the musical notes in the musical score display. By moving a cursor, musical practitioners select a musical note, and after selecting it, by clicking "Play" button in the "Playback" field, they can playback from the selected note onward.

Figure 13. A picture describing how music practitioners can utilize a system based on our method. A pianist sits at the piano and presses some keys. In front of them and on the piano, there is a laptop computer that displays the musical score based user interface based on our method. The pianist makes trial and errors by reviewing the performance with the system that highlight several notes of their performance.